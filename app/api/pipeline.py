# app/api/pipeline.py
import asyncio
import io
import os
import csv
import json
import traceback
import uuid
import tempfile
from pathlib import Path
from typing import Any, Tuple, Optional

import numpy as np
import pandas as pd
from fastapi import APIRouter, UploadFile, File, Form
from fastapi.responses import StreamingResponse

from app.core.preprocess import run_preprocess, filter_df_before_year, get_cpc_path
from app.core.embedding import run_embedding
from app.core.clustering import run_clustering
from app.core.tech_naming import run_tech_naming
from app.core.component_tech_grouper import run_component_grouping, ComponentTechConfig
from app.core.component_tech_naming import generate_component_names_csv

router = APIRouter(tags=["Pipeline"])

# ============================================================
# í™˜ê²½ì„¤ì •
# ============================================================
OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "/var/lib/app/outputs")).resolve()
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------- ìœ í‹¸: í¬ë§· ìŠ¤ë‹ˆí•‘ ----------------------
def _looks_like_json(txt: str) -> bool:
    s = txt.lstrip()
    return s.startswith("{") or s.startswith("[")

def _looks_like_jsonl(txt: str) -> bool:
    lines = [ln.strip() for ln in txt.splitlines() if ln.strip()]
    sample = lines[: min(5, len(lines))]
    return len(sample) > 1 and all(ln.startswith("{") or ln.startswith("[") for ln in sample)

# ---------------------- ë¡œë”: íŒŒì¼ ê²½ë¡œ â†’ DataFrame ----------------------
def _load_table_from_path(
    file_path: Path,
    prefer_jsonl: bool = True,
) -> Tuple[pd.DataFrame, dict]:
    name = file_path.name.lower()
    meta: dict = {"filename": file_path.name}

    # Excel
    if name.endswith((".xlsx", ".xls")):
        df_xlsx = pd.read_excel(str(file_path), engine="openpyxl")
        meta.update({"format": "excel"})
        return df_xlsx, meta

    # Sniff small bytes
    sniff_bytes = file_path.read_bytes()[:100_000]
    sniff_text, sniff_enc = None, None
    for enc in ["utf-8-sig", "utf-8", "latin-1", "cp949", "euc-kr"]:
        try:
            sniff_text = sniff_bytes.decode(enc, errors="strict")
            sniff_enc = enc
            break
        except Exception:
            continue

    if sniff_text:
        if prefer_jsonl and _looks_like_jsonl(sniff_text):
            try:
                df = pd.read_json(str(file_path), lines=True)
                meta.update({"format": "jsonl-sniff", "encoding": sniff_enc})
                return df, meta
            except Exception:
                pass
        if _looks_like_json(sniff_text):
            try:
                with file_path.open("r", encoding=sniff_enc or "utf-8") as f:
                    obj = json.load(f)
                if isinstance(obj, list):
                    meta.update({"format": "json-sniff", "encoding": sniff_enc})
                    return pd.DataFrame(obj), meta
                elif isinstance(obj, dict):
                    for k in ("data", "items", "records", "rows", "result"):
                        if k in obj and isinstance(obj[k], list):
                            meta.update({"format": "json-sniff", "encoding": sniff_enc, "root": k})
                            return pd.DataFrame(obj[k]), meta
                    meta.update({"format": "json-sniff", "encoding": sniff_enc, "normalized": True})
                    return pd.json_normalize(obj), meta
            except Exception:
                pass

    # í™•ì¥ì ì§íŒ
    if name.endswith((".jsonl", ".ndjson")):
        df = pd.read_json(str(file_path), lines=True)
        meta.update({"format": "jsonl"})
        return df, meta
    if name.endswith(".json"):
        with file_path.open("r", encoding=sniff_enc or "utf-8") as f:
            obj = json.load(f)
        if isinstance(obj, list):
            meta.update({"format": "json"})
            return pd.DataFrame(obj), meta
        elif isinstance(obj, dict):
            for k in ("data", "items", "records", "rows", "result"):
                if k in obj and isinstance(obj[k], list):
                    meta.update({"format": "json", "root": k})
                    return pd.DataFrame(obj[k]), meta
            meta.update({"format": "json", "normalized": True})
            return pd.json_normalize(obj), meta

    # CSV/TSV
    try:
        dialect = csv.Sniffer().sniff(sniff_bytes.decode("utf-8", errors="ignore"), delimiters=",;\t|")
        sep_guess = dialect.delimiter
    except Exception:
        sep_guess = None
    try:
        df = pd.read_csv(str(file_path), sep=sep_guess, engine="c", quotechar='"', escapechar="\\", on_bad_lines="error")
        meta.update({"format": "csv", "sep": sep_guess, "engine": "c"})
        return df, meta
    except Exception:
        pass
    df = pd.read_csv(str(file_path), sep=sep_guess, engine="python", quotechar='"', escapechar="\\", on_bad_lines="skip")
    meta.update({"format": "csv", "sep": sep_guess, "engine": "python", "on_bad_lines": "skip"})
    return df, meta

# ---------------------- ì•ˆì „ ë¦¬ìŠ¤íŠ¸ ë³€í™˜ ----------------------
def _to_str_list(x: Any) -> list[str]:
    if x is None:
        return []
    if isinstance(x, (list, tuple)):
        return [str(v) for v in x]
    if isinstance(x, pd.Series):
        return x.dropna().astype(str).tolist()
    if isinstance(x, pd.DataFrame):
        return x.astype(str).stack().tolist()
    return [str(x)]

# ---------------------- ì—…ë¡œë“œ ìŠ¤íŠ¸ë¦¬ë° â†’ ì„ì‹œíŒŒì¼ ----------------------
async def _save_upload_to_tempfile(file: UploadFile) -> Path:
    # ì—…ë¡œë“œë˜ëŠ” íŒŒì¼ í™•ì¥ì ë³´ì¡´(ì§„ë‹¨ í¸ì˜)
    suffix = ""
    if file.filename and "." in file.filename:
        suffix = "." + file.filename.rsplit(".", 1)[-1]
    tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
    tmp_path = Path(tmp.name)
    try:
        # 1MB ì²­í¬ë¡œ ë³µì‚¬(ë©”ëª¨ë¦¬ í­ì£¼ ë°©ì§€)
        while True:
            chunk = await file.read(1024 * 1024)
            if not chunk:
                break
            tmp.write(chunk)
        tmp.flush()
        tmp.close()
        return tmp_path
    except Exception:
        try:
            tmp.close()
        finally:
            tmp_path.unlink(missing_ok=True)
        raise


@router.post("/pipeline/run")
async def run_pipeline(
    file: UploadFile = File(...),
    cutoff_year: int = Form(2025),
    n_clusters: int = Form(100),
    model_name: str = Form("all-MiniLM-L6-v2"),
    top_n: int = Form(100),
    run_id: str = Form(None),
):
    """
    ì˜êµ¬ ì €ì¥ ì—†ì´ ì²˜ë¦¬:
      - ì—…ë¡œë“œëŠ” ì„ì‹œíŒŒì¼ì—ë§Œ ì €ì¥(ìš”ì²­ ëë‚˜ë©´ ì‚­ì œ)
      - ì¤‘ê°„ ì‚°ì¶œë¬¼ë„ ë©”ëª¨ë¦¬/ì„ì‹œíŒŒì¼ë¡œë§Œ ì‚¬ìš©
      - ì§„í–‰ìƒí™©ì„ JSON ë¼ì¸ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë°
      - ê²°ê³¼ë¬¼: OUTPUT_DIRì— CSV 2ê°œ ì €ì¥
          * ìš”ì†Œê¸°ìˆ  ë„¤ì´ë°: names_generated_flowagg.csv
          * êµ¬ì„±ê¸°ìˆ  ë„¤ì´ë°: component_tech_names.csv
    """
    rid = run_id or uuid.uuid4().hex
    temp_paths: list[Path] = []

    def j(obj):
        """JSON í•œ ì¤„ ìŠ¤íŠ¸ë¦¼ í¬ë§·"""
        return json.dumps(obj, ensure_ascii=False) + "\n"

    async def stream():
        try:
            # 0) ì—…ë¡œë“œ â†’ ì„ì‹œíŒŒì¼
            yield j({"step": "íŒŒì¼ ì €ì¥ ì‹œì‘", "progress": 0})
            src_path = await _save_upload_to_tempfile(file)
            temp_paths.append(src_path)
            yield j({
                "step": "íŒŒì¼ ì €ì¥ ì™„ë£Œ",
                "progress": 4,
                "meta": {"path": str(src_path), "filename": file.filename}
            })

            # 1) íŒŒì¼ ë¡œë“œ
            yield j({"step": "íŒŒì¼ ë¡œë“œ ì¤‘", "progress": 5})
            df, meta = await asyncio.to_thread(_load_table_from_path, src_path, True)
            yield j({
                "step": "íŒŒì¼ ë¡œë“œ ì™„ë£Œ",
                "progress": 10,
                "meta": {"filename": file.filename, **meta}
            })
            print(f"âœ… ì „ì²˜ë¦¬ ì‹œì‘: {len(df):,}ê°œì˜ ë°ì´í„°")

            # 2) ì—°ë„ í•„í„°ë§
            yield j({"step": "ë°ì´í„° ì—°ë„ í•„í„°ë§ ì‹œì‘", "progress": 15})
            df_year = await asyncio.to_thread(filter_df_before_year, df, int(cutoff_year))
            print(f"âœ… ì—°ë„ í•„í„°ë§ ì™„ë£Œ: {len(df_year):,}ê°œì˜ ë°ì´í„°")

            # 3) ì „ì²˜ë¦¬
            yield j({"step": "ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘", "progress": 20})
            df_clean = await asyncio.to_thread(
                run_preprocess,
                df_year,
                int(cutoff_year),
                do_cpc_match=True,
                cpc_csv_path=get_cpc_path(),
            )
            print(f"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_clean):,}ê°œì˜ ë°ì´í„°")

            # 4) ì„ë² ë”© (í•˜íŠ¸ë¹„íŠ¸ í¬í•¨)
            yield j({"step": "ì„ë² ë”© ì¤‘", "progress": 40})
            progress_q: asyncio.Queue = asyncio.Queue()
            loop = asyncio.get_event_loop()
            last_progress: Optional[tuple[int, int]] = None

            def _progress_cb(processed: int, total: int):
                try:
                    loop.call_soon_threadsafe(progress_q.put_nowait, (processed, total))
                except Exception:
                    pass

            # ì„ë² ë”© ì‹¤í–‰
            task_embed = asyncio.create_task(asyncio.to_thread(
                run_embedding,
                df_clean,
                model_name,
                batch_size=512,
                checkpoint_dir=f"/tmp/emb_ckpt/{rid}",
                resume=True,
                progress_cb=_progress_cb,
            ))

            # í•˜íŠ¸ë¹„íŠ¸
            HB_INTERVAL = 2
            while not task_embed.done():
                try:
                    await asyncio.sleep(HB_INTERVAL)
                    while True:
                        last_progress = progress_q.get_nowait()
                except asyncio.QueueEmpty:
                    pass

                if last_progress is not None:
                    processed, total = last_progress
                    pct = 40 + (processed / max(total, 1)) * 30
                    yield j({
                        "step": "ping",
                        "progress": int(pct),
                        "meta": {
                            "stage": "embedding",
                            "processed": processed,
                            "total": total,
                            "batch_size": 512
                        }
                    })
                else:
                    yield j({"step": "ping", "progress": 41})

            # ì„ë² ë”© ì™„ë£Œ
            df_embed = await task_embed

            # 5) í´ëŸ¬ìŠ¤í„°ë§/ìš”ì•½
            yield j({"step": "í´ëŸ¬ìŠ¤í„°ë§ ë° ì¶”ì„¸ ë¶„ì„ ì¤‘", "progress": 70})
            df_clustered, summary = await asyncio.to_thread(run_clustering, df_embed, n_clusters)
            if not isinstance(summary, dict) or "artifacts" not in summary:
                raise RuntimeError("artifacts ëˆ„ë½")

            # 6) ìš”ì†Œê¸°ìˆ  ë„¤ì´ë°
            yield j({"step": "ê¸°ìˆ ëª… ìƒì„± ì¤‘", "progress": 85})
            naming_result = await asyncio.to_thread(
                run_tech_naming, None, artifacts=summary["artifacts"], top_n=int(top_n)
            )
            elem_csv_path = naming_result["paths"].get("flowagg_csv", "")

            # 7) êµ¬ì„±ê¸°ìˆ  ë¬¶ê¸°
            yield j({"step": "êµ¬ì„±ê¸°ìˆ  ë¬¶ëŠ” ì¤‘", "progress": 90})
            cfg = ComponentTechConfig(
                n_components=int(n_clusters),
                year_col="year",
                embed_col="embedding",
                cluster_col=summary["paths"]["label_col"],
                random_state=42,
            )
            df_component, comp_summary = await asyncio.to_thread(run_component_grouping, df_clustered, cfg)

            # 8) êµ¬ì„±ê¸°ìˆ  ë„¤ì´ë°
            yield j({"step": "êµ¬ì„±ê¸°ìˆ  ë„¤ì´ë° ì¤‘", "progress": 96})
            comp_csv_path = await asyncio.to_thread(
                generate_component_names_csv,
                df_component,
                label_col="component_tech_id",
                text_cols=("title",),
                output_csv_path=None,
            )

            # 9) ì™„ë£Œ
            keywords = _to_str_list(summary.get("keywords", []))[:100]
            titles = _to_str_list(summary.get("titles", []))[:100]
            yield j({
                "step": "ì™„ë£Œ",
                "progress": 100,
                "result": {
                    "outputs": {
                        "element_names_csv": elem_csv_path,
                        "component_names_csv": comp_csv_path,
                    },
                    "summary": {
                        "keywords": keywords,
                        "titles": titles,
                        "paths": summary.get("paths", {})
                    },
                    "run_id": rid,
                },
            })

        except asyncio.CancelledError:
            raise
        except Exception as e:
            tb = traceback.format_exc()
            print("[STREAM ERROR]", tb)
            yield j({
                "step": "ì˜¤ë¥˜ ë°œìƒ",
                "progress": -1,
                "error": str(e),
                "traceback": tb[-1000:],
            })
        finally:
            try:
                yield j({"step": "stream-close", "progress": -2})
            except Exception:
                pass
            # ì„ì‹œíŒŒì¼ ì •ë¦¬
            for p in temp_paths:
                try:
                    p.unlink(missing_ok=True)
                except Exception:
                    pass

    # --- Streaming Response ì„¤ì • ---
    return StreamingResponse(
        stream(),
        media_type="text/event-stream",  # ğŸ”¹ ìŠ¤íŠ¸ë¦¬ë° ì•ˆì •ì„± í–¥ìƒ
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # ğŸ”¹ NGINX ë²„í¼ë§ ë°©ì§€
        },
    )
